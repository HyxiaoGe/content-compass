import { createClient } from '@supabase/supabase-js';
import type { Database } from '@/types/database-v2';
import { ContentAnalyzer } from '../ai/content-analyzer';
import { WebScraper } from './web-scraper';

/**
 * äº§å“æ›´æ–°çˆ¬å–å™¨
 * 
 * åŠŸèƒ½ï¼š
 * 1. ä»å„AIäº§å“å®˜ç½‘çˆ¬å–æœ€æ–°æ›´æ–°ä¿¡æ¯
 * 2. ä½¿ç”¨AIåˆ†æå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
 * 3. å°†ç»“æ„åŒ–æ•°æ®å­˜å‚¨åˆ°æ•°æ®åº“
 */

interface CrawlConfig {
  name: string;
  homepage: string;
  changelogUrl?: string;
  blogUrl?: string;
  rssUrl?: string;
  selectors: {
    title?: string;
    content?: string;
    date?: string;
    version?: string;
  };
  userAgent?: string;
  rateLimit?: number; // è¯·æ±‚é—´éš”ï¼ˆæ¯«ç§’ï¼‰
}

interface RawUpdate {
  title: string;
  content: string;
  url: string;
  publishDate?: Date;
  version?: string;
  source: 'changelog' | 'blog' | 'rss';
}

interface ProcessedUpdate {
  title: string;
  summary: string;
  keyPoints: string[];
  importance: 'high' | 'medium' | 'low';
  tags: string[];
  version?: string;
  publishDate: Date;
  originalUrl: string;
  confidence: number;
}

export class ProductCrawler {
  private supabase;
  private contentAnalyzer: ContentAnalyzer;
  private webScraper: WebScraper;

  constructor() {
    // åˆå§‹åŒ–Supabaseå®¢æˆ·ç«¯
    this.supabase = createClient<Database>(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY! || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
    );
    
    // åˆå§‹åŒ–AIå†…å®¹åˆ†æå™¨
    this.contentAnalyzer = new ContentAnalyzer();
    
    // åˆå§‹åŒ–ç½‘é¡µçˆ¬å–å™¨
    this.webScraper = new WebScraper();
  }

  /**
   * çˆ¬å–å•ä¸ªäº§å“çš„æ›´æ–°ä¿¡æ¯
   */
  async crawlProduct(productSlug: string): Promise<{
    success: boolean;
    updatesFound: number;
    newUpdates: number;
    error?: string;
  }> {
    try {
      console.log(`ğŸ” å¼€å§‹çˆ¬å–äº§å“: ${productSlug}`);

      // 1. è·å–äº§å“é…ç½®
      const { data: product, error: productError } = await this.supabase
        .from('ai_products')
        .select('*')
        .eq('slug', productSlug)
        .eq('is_active', true)
        .single();

      if (productError || !product) {
        throw new Error(`äº§å“ä¸å­˜åœ¨æˆ–æœªå¯ç”¨: ${productSlug}`);
      }

      // 2. æ„å»ºçˆ¬å–é…ç½®
      const config = this.buildCrawlConfig(product);
      
      // 3. æ‰§è¡Œçˆ¬å–
      const rawUpdates = await this.fetchUpdates(config);
      console.log(`ğŸ“„ å‘ç° ${rawUpdates.length} æ¡åŸå§‹æ›´æ–°`);

      // 4. è¿‡æ»¤æ–°å†…å®¹
      const newUpdates = await this.filterNewUpdates(product.id, rawUpdates);
      console.log(`ğŸ†• å…¶ä¸­ ${newUpdates.length} æ¡ä¸ºæ–°å†…å®¹`);

      if (newUpdates.length === 0) {
        return {
          success: true,
          updatesFound: rawUpdates.length,
          newUpdates: 0
        };
      }

      // 5. AIå¤„ç†å†…å®¹
      const processedUpdates = await this.processUpdatesWithAI(newUpdates, product.name);
      console.log(`ğŸ¤– AIå¤„ç†å®Œæˆ ${processedUpdates.length} æ¡æ›´æ–°`);

      // 6. å­˜å‚¨åˆ°æ•°æ®åº“
      const savedCount = await this.saveUpdates(product.id, processedUpdates);
      console.log(`ğŸ’¾ å·²ä¿å­˜ ${savedCount} æ¡æ›´æ–°åˆ°æ•°æ®åº“`);

      // 7. æ›´æ–°äº§å“çŠ¶æ€
      await this.updateProductCrawlStatus(product.id, 'success');

      return {
        success: true,
        updatesFound: rawUpdates.length,
        newUpdates: savedCount
      };

    } catch (error) {
      console.error(`âŒ çˆ¬å–äº§å“ ${productSlug} å¤±è´¥:`, error);
      
      // æ›´æ–°é”™è¯¯çŠ¶æ€
      try {
        const { data: product } = await this.supabase
          .from('ai_products')
          .select('id')
          .eq('slug', productSlug)
          .single();
          
        if (product) {
          await this.updateProductCrawlStatus(
            product.id, 
            'error', 
            error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
          );
        }
      } catch (updateError) {
        console.error('æ›´æ–°é”™è¯¯çŠ¶æ€å¤±è´¥:', updateError);
      }

      return {
        success: false,
        updatesFound: 0,
        newUpdates: 0,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      };
    }
  }

  /**
   * æ„å»ºçˆ¬å–é…ç½®
   */
  private buildCrawlConfig(product: any): CrawlConfig {
    // åŸºç¡€é…ç½®
    const config: CrawlConfig = {
      name: product.name,
      homepage: product.homepage_url,
      selectors: {},
      userAgent: 'Mozilla/5.0 (compatible; ContentCompass/1.0; +https://content-compass.netlify.app)',
      rateLimit: 2000 // 2ç§’é—´éš”
    };

    // æ ¹æ®äº§å“å®šåˆ¶é…ç½®
    switch (product.slug) {
      case 'openai':
        config.blogUrl = 'https://openai.com/blog';
        config.selectors = {
          title: 'h2 a',
          content: '.post-excerpt',
          date: '.post-date'
        };
        break;
        
      case 'github-copilot':
        config.changelogUrl = 'https://github.com/github/copilot-docs/releases';
        config.selectors = {
          title: '.release-title a',
          content: '.markdown-body',
          date: 'relative-time',
          version: '.tag-name'
        };
        break;
        
      case 'cursor':
        config.changelogUrl = 'https://cursor.sh/changelog';
        config.selectors = {
          title: '.changelog-title',
          content: '.changelog-content',
          date: '.changelog-date',
          version: '.version-tag'
        };
        break;
        
      case 'claude':
        config.blogUrl = 'https://www.anthropic.com/news';
        config.selectors = {
          title: '.news-title a',
          content: '.news-excerpt',
          date: '.news-date'
        };
        break;
        
      default:
        // ä½¿ç”¨é€šç”¨é…ç½®
        config.changelogUrl = product.changelog_url;
        break;
    }

    return config;
  }

  /**
   * è·å–æ›´æ–°ä¿¡æ¯
   */
  private async fetchUpdates(config: CrawlConfig): Promise<RawUpdate[]> {
    const updates: RawUpdate[] = [];

    try {
      // 1. å°è¯•çˆ¬å–Changelogé¡µé¢
      if (config.changelogUrl) {
        const changelogUpdates = await this.scrapeChangelog(config);
        updates.push(...changelogUpdates);
      }

      // 2. å°è¯•çˆ¬å–åšå®¢é¡µé¢
      if (config.blogUrl && updates.length < 5) {
        const blogUpdates = await this.scrapeBlog(config);
        updates.push(...blogUpdates);
      }

      // 3. å°è¯•çˆ¬å–RSS feed
      if (config.rssUrl && updates.length < 5) {
        const rssUpdates = await this.scrapeRSS(config);
        updates.push(...rssUpdates);
      }

      console.log(`ğŸ“„ ${config.name} çˆ¬å–å®Œæˆï¼Œå…±å‘ç° ${updates.length} æ¡æ›´æ–°`);
      
      return updates;

    } catch (error) {
      console.error(`âŒ çˆ¬å– ${config.name} å¤±è´¥:`, error);
      
      // å¦‚æœçˆ¬å–å¤±è´¥ï¼Œè¿”å›ç©ºæ•°ç»„è€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯
      return [];
    }
  }

  /**
   * çˆ¬å–Changelogé¡µé¢
   */
  private async scrapeChangelog(config: CrawlConfig): Promise<RawUpdate[]> {
    if (!config.changelogUrl) return [];

    try {
      console.log(`ğŸ“‹ çˆ¬å– ${config.name} changelog: ${config.changelogUrl}`);
      
      const content = await this.webScraper.scrape(config.changelogUrl, {
        userAgent: config.userAgent,
        timeout: 30000,
        javascript: config.changelogUrl.includes('github.com') // GitHubéœ€è¦JSæ¸²æŸ“
      });

      const extractedData = this.webScraper.extractData(content.html, {
        titles: config.selectors.title || 'h1, h2, h3, .title, .changelog-title',
        contents: config.selectors.content || 'p, .content, .description, .changelog-content',
        dates: config.selectors.date || 'time, .date, .publish-date, .changelog-date',
        versions: config.selectors.version || '.version, .tag, .version-tag'
      });

      return this.parseExtractedData(extractedData, config.changelogUrl, 'changelog');

    } catch (error) {
      console.error(`âŒ çˆ¬å– ${config.name} changelog å¤±è´¥:`, error);
      return [];
    }
  }

  /**
   * çˆ¬å–åšå®¢é¡µé¢
   */
  private async scrapeBlog(config: CrawlConfig): Promise<RawUpdate[]> {
    if (!config.blogUrl) return [];

    try {
      console.log(`ğŸ“° çˆ¬å– ${config.name} blog: ${config.blogUrl}`);
      
      const content = await this.webScraper.scrape(config.blogUrl, {
        userAgent: config.userAgent,
        timeout: 30000,
        javascript: false
      });

      const extractedData = this.webScraper.extractData(content.html, {
        titles: config.selectors.title || 'h1, h2, h3, .title, .post-title, .article-title',
        contents: config.selectors.content || 'p, .excerpt, .summary, .post-excerpt',
        dates: config.selectors.date || 'time, .date, .publish-date, .post-date',
        links: 'a[href]'
      });

      return this.parseExtractedData(extractedData, config.blogUrl, 'blog');

    } catch (error) {
      console.error(`âŒ çˆ¬å– ${config.name} blog å¤±è´¥:`, error);
      return [];
    }
  }

  /**
   * çˆ¬å–RSS feed
   */
  private async scrapeRSS(config: CrawlConfig): Promise<RawUpdate[]> {
    if (!config.rssUrl) return [];

    try {
      console.log(`ğŸ“¡ çˆ¬å– ${config.name} RSS: ${config.rssUrl}`);
      
      const content = await this.webScraper.scrape(config.rssUrl, {
        userAgent: config.userAgent,
        timeout: 30000,
        javascript: false
      });

      // è§£æRSS XML
      const extractedData = this.webScraper.extractData(content.html, {
        titles: 'item title, entry title',
        contents: 'item description, entry summary, entry content',
        dates: 'item pubDate, entry published, item published',
        links: 'item link, entry link'
      });

      return this.parseExtractedData(extractedData, config.rssUrl, 'rss');

    } catch (error) {
      console.error(`âŒ çˆ¬å– ${config.name} RSS å¤±è´¥:`, error);
      return [];
    }
  }

  /**
   * è§£ææå–çš„æ•°æ®
   */
  private parseExtractedData(
    data: Record<string, string[]>, 
    baseUrl: string, 
    source: 'changelog' | 'blog' | 'rss'
  ): RawUpdate[] {
    const updates: RawUpdate[] = [];
    const maxItems = Math.min(data.titles?.length || 0, 10); // æœ€å¤šå¤„ç†10ä¸ªé¡¹ç›®

    for (let i = 0; i < maxItems; i++) {
      const title = data.titles?.[i];
      const content = data.contents?.[i];
      
      if (!title || !content) continue;

      // è§£æå‘å¸ƒæ—¥æœŸ
      let publishDate: Date | undefined;
      const dateStr = data.dates?.[i];
      if (dateStr) {
        publishDate = this.parseDate(dateStr);
      }

      // è§£æé“¾æ¥
      let url = baseUrl;
      const linkStr = data.links?.[i];
      if (linkStr) {
        url = this.webScraper.resolveUrl(baseUrl, linkStr);
      }

      // æå–ç‰ˆæœ¬å·
      let version: string | undefined;
      const versionStr = data.versions?.[i];
      if (versionStr) {
        version = this.extractVersion(versionStr);
      }

      updates.push({
        title: this.webScraper.cleanText(title),
        content: this.webScraper.cleanText(content),
        url,
        publishDate,
        version,
        source
      });
    }

    return updates;
  }

  /**
   * è§£ææ—¥æœŸå­—ç¬¦ä¸²
   */
  private parseDate(dateStr: string): Date | undefined {
    try {
      // å°è¯•ç›´æ¥è§£æ
      let date = new Date(dateStr);
      if (!isNaN(date.getTime())) {
        return date;
      }

      // å°è¯•è§£æç›¸å¯¹æ—¶é—´
      const relativeMatch = dateStr.match(/(\d+)\s*(days?|weeks?|months?|years?)\s*ago/i);
      if (relativeMatch) {
        const [, num, unit] = relativeMatch;
        const now = new Date();
        const amount = parseInt(num);
        
        switch (unit.toLowerCase()) {
          case 'day':
          case 'days':
            return new Date(now.getTime() - amount * 24 * 60 * 60 * 1000);
          case 'week':
          case 'weeks':
            return new Date(now.getTime() - amount * 7 * 24 * 60 * 60 * 1000);
          case 'month':
          case 'months':
            return new Date(now.getTime() - amount * 30 * 24 * 60 * 60 * 1000);
          case 'year':
          case 'years':
            return new Date(now.getTime() - amount * 365 * 24 * 60 * 60 * 1000);
        }
      }

      return undefined;
    } catch {
      return undefined;
    }
  }

  /**
   * æå–ç‰ˆæœ¬å·
   */
  private extractVersion(versionStr: string): string | undefined {
    const versionMatch = versionStr.match(/v?(\d+\.\d+(?:\.\d+)?(?:-\w+)?)/i);
    return versionMatch ? versionMatch[1] : undefined;
  }

  /**
   * è¿‡æ»¤æ–°å†…å®¹ï¼ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼‰
   */
  private async filterNewUpdates(productId: string, updates: RawUpdate[]): Promise<RawUpdate[]> {
    const newUpdates: RawUpdate[] = [];

    for (const update of updates) {
      // ç”Ÿæˆå†…å®¹å“ˆå¸Œ
      const contentHash = this.generateContentHash(update.title + update.content);
      
      // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
      const { data: existing } = await this.supabase
        .from('product_updates')
        .select('id')
        .eq('product_id', productId)
        .eq('content_hash', contentHash)
        .single();

      if (!existing) {
        newUpdates.push(update);
      }
    }

    return newUpdates;
  }

  /**
   * ä½¿ç”¨AIå¤„ç†æ›´æ–°å†…å®¹
   */
  private async processUpdatesWithAI(updates: RawUpdate[], productName: string): Promise<ProcessedUpdate[]> {
    const processed: ProcessedUpdate[] = [];

    console.log(`ğŸ¤– å¼€å§‹AIåˆ†æ ${updates.length} æ¡æ›´æ–°`);

    for (const update of updates) {
      try {
        // è°ƒç”¨AIå†…å®¹åˆ†æå™¨
        const analysisResult = await this.contentAnalyzer.analyzeContent({
          title: update.title,
          content: update.content,
          url: update.url,
          publishDate: update.publishDate,
          productName
        });

        const processedUpdate: ProcessedUpdate = {
          title: analysisResult.title,
          summary: analysisResult.summary,
          keyPoints: analysisResult.keyPoints,
          importance: analysisResult.importance,
          tags: analysisResult.tags,
          version: update.version,
          publishDate: update.publishDate || new Date(),
          originalUrl: update.url,
          confidence: analysisResult.confidence
        };

        processed.push(processedUpdate);
        
        console.log(`âœ… AIåˆ†æå®Œæˆ: ${update.title} (ç½®ä¿¡åº¦: ${analysisResult.confidence})`);
        
      } catch (error) {
        console.error(`âŒ AIåˆ†æå¤±è´¥: ${update.title}`, error);
        
        // ä½¿ç”¨é™çº§å¤„ç†
        const fallbackUpdate: ProcessedUpdate = {
          title: update.title,
          summary: update.content.length > 200 ? update.content.substring(0, 200) + '...' : update.content,
          keyPoints: ['äº§å“åŠŸèƒ½æ›´æ–°'],
          importance: 'medium',
          tags: ['æ›´æ–°'],
          version: update.version,
          publishDate: update.publishDate || new Date(),
          originalUrl: update.url,
          confidence: 0.3
        };
        
        processed.push(fallbackUpdate);
      }
    }

    return processed;
  }

  /**
   * ä¿å­˜æ›´æ–°åˆ°æ•°æ®åº“
   */
  private async saveUpdates(productId: string, updates: ProcessedUpdate[]): Promise<number> {
    let savedCount = 0;

    for (const update of updates) {
      try {
        const { error } = await this.supabase
          .from('product_updates')
          .insert({
            product_id: productId,
            title: update.title,
            summary: update.summary,
            key_points: update.keyPoints,
            importance_level: update.importance,
            tags: update.tags,
            version_number: update.version,
            original_url: update.originalUrl,
            published_at: update.publishDate.toISOString(),
            status: 'published',
            content_hash: this.generateContentHash(update.title + update.summary),
            confidence_score: update.confidence,
            ai_model_used: 'gpt-4',
            scraped_at: new Date().toISOString(),
            processed_at: new Date().toISOString()
          });

        if (!error) {
          savedCount++;
        } else {
          console.error('ä¿å­˜æ›´æ–°å¤±è´¥:', error);
        }
      } catch (error) {
        console.error('ä¿å­˜æ›´æ–°å¼‚å¸¸:', error);
      }
    }

    return savedCount;
  }

  /**
   * æ›´æ–°äº§å“çˆ¬å–çŠ¶æ€
   */
  private async updateProductCrawlStatus(
    productId: string, 
    status: 'success' | 'error', 
    error?: string
  ): Promise<void> {
    await this.supabase
      .from('ai_products')
      .update({
        last_crawled_at: new Date().toISOString(),
        crawl_status: status,
        crawl_error: error || null
      })
      .eq('id', productId);
  }

  /**
   * ç”Ÿæˆå†…å®¹å“ˆå¸Œ
   */
  private generateContentHash(content: string): string {
    // ç®€å•çš„å“ˆå¸Œå‡½æ•°ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ›´å¼ºçš„å“ˆå¸Œç®—æ³•
    let hash = 0;
    for (let i = 0; i < content.length; i++) {
      const char = content.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // è½¬æ¢ä¸º32ä½æ•´æ•°
    }
    return Math.abs(hash).toString(36);
  }
}